---
title: "Tutorial 3"
format: 
  revealjs:
    theme: theme_mm.scss
    menu:
      side: left
    slide-number: c
    date-format: long
    embed-resources: true
toc: false
visual: true
toc-title: ""
toc-location: right
author:
  - name: Mat√©o Moglia
    email: mateo.moglia@gmail.com
    affiliations: CREST - Institut Polytechnique de Paris
    url: https://mateomoglia.github.io
date: "March 5, 2025"
execute:
  cache: true
---

```{r, echo=F,eval=T,message=F,warning=F}
packages <- c("ggplot2", "readxl", "dplyr", "tidyr","xtable","patchwork","datasets")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}
# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
```

## Roadmap

For this third tutorial, we are going to

1.  Recap on the OLS estimation
2.  Learn how to go from a model to an empirical prediction
3.  Replicate the key results from Mankiw, Romer, Weil (1994)

::: callout-note
Big disclaimer: the recap on OLS is hugely inspired from Florian Oswald's and Sciences Po's introduction to Econometrics with R. The interested student can check this fantastical material here: [scpoecon.github.io](scpoecon.github.io).
:::

## Linear regression

### Why? When?

1.  An empirical tool to assess the statistical relationship between variables
2.  In the toolbox of the social scientist, with many other tools
3.  Correlation **is not** causation

Topic of today: **Ordinary Least Square** estimator (which is a widely used estimator)

::: callout-tip
Keep in mind the difference between an estimator $\hat \beta$ and the observed value $\beta$. We cannot observe the true parameter, so we estimate it (with **error**).
:::

## Linear regression

### Visual intuition

We use the `cars` dataset, in base R, and we are going to relate how speed and stopping distance are related.

```{r}
ggplot(cars,aes(x=speed,y=dist)) +
  geom_point(color="red") +
  xlab("Speed") + ylab("Stopping distance") +
  ggtitle("Car speed vs. stopping distance") +
  theme_bw() 
```

## Linear regression

### Visual intuition

It seems that a line could be helpful to "summarize" the relation between both variables!

```{r}
ggplot(cars,aes(x=speed,y=dist)) +
  geom_point(color="red") +
  geom_abline(intercept=10,slope=2.5,color="blue") +
  xlab("Speed") + ylab("Stopping distance") +
  ggtitle("Car speed vs. stopping distance") +
  theme_bw() 
```

## Linear regression

### Bringing some maths

-   We want to minimize the distance between the line and the points
-   In practice, the sum of distances can go to 0. Solution?

```{r}
ggplot(cars, aes(x = speed, y = dist)) +
  geom_point(color = "black") +
  geom_abline(intercept = 5, slope = 2.5, color = "blue") +
  geom_segment(aes(x = speed, 
                   y = dist, 
                   xend = speed, 
                   yend = 5 + 2.5 * speed), 
               arrow = arrow(length = unit(0.1, "inches")), 
               color = "red") +
  xlab("Speed") + 
  ylab("Stopping distance") +
  ggtitle("Car speed vs. stopping distance") +
  theme_bw()
```

## Linear regression

### Bringing some maths

-   We compute the sum of square distances ($\sim$ error), so the name "ordinary least square"

-   An affine function is defined as $y = \beta_0 + \beta_1 x$ which in matrix form gives $Y = X\beta$.

-   Dimension of $X$?

-   The error for each observation is $e_i = y_i - \beta_0 - \beta_1 x_i$

-   Hence, we look for $\beta$ such that: $$
    \beta^\star = \min_\beta (Y - X\beta)'(Y - X\beta) = \min_\beta e'e
    $$

-   First order condition: derivative wrt $\beta$ should be equal to 0

## Linear regression

### Defining the OLS estimator

-   The OLS estimator is then: $$
    \hat{\beta} = (X'X)^{-1}X'y
    $$

-   Who wants to try to derive it?

-   Under some conditions, this estimator is the **Best Linear Unbiased Estimator** (BLUE)

-   Those conditions will be super helpful later

## Linear regression

### Gauss-Markov theorem

1.  Linearity
    -   The relationship has to be linear
    -   See <a href="#appendix" class="btn btn-primary">Anscombe quartet</a>
    -   Solutions: $\log$, squared
2.  $X$ is full rank
    -   Ensure that variables are not perfectly colinear
    -   There are more observations than variables ($n > k$)
3.  Zero conditional mean
    -   Errors are not correlated with $X$ (**exogeneity**)
4.  Homoskedasticity
    -   Errors are not correlated between observations
    -   Solutions: robust estimation

## Linear regression

### Gauss-Markov theorem violation

Major threat is 3 (= **endogeneity**). Usually it arises from 3 sources:

1.  Reverse causality. Any exemple?
2.  Omitted Variable Bias. Any exemple?
3.  Measurement error. Any exemple?

Then, the OLS estimator is **biased**... We will see many ways to tackle this issue.

-   Today: **Omitted Variable Bias**.

## Linear regression

### R2 and prediction quality

-   R2 is the coefficient of determination, assessing the share of total variance explained by the model

$$\small 
R^2 = \frac{\texttt{Variance explained}}{\texttt{Total variance}} = \frac{\text{Sum of explained square}}{\text{Total sum of square}} = 1 - \frac{\text{Sum of square residuals}}{\text{Total sum of square}}
$$

-   R2 is between 0 and 1
-   The higher the R2, the larger the share of variance explained by the model
-   A low R2 is not necessarily bad: it is just that the model explains a low share of variance

## Linear regression

### Interpretation

-   Once the model set, we can estimate it using R functions
-   Main function is `lm`. For linear model, the syntax is `lm(y ~ x)`

```{r, echo=T}
summary(lm(cars$speed ~ cars$dist))
```

-   Results are significant (low p-value), R2 is relatively large

## Linear regression

### Interpretation

::: callout-note
Everything should be intepreted **everything else equal**!
:::

-   Level-level regression: marginal effect. If $x$ increases by 1, $y$ increases by $\beta$

-   Log-log regression: elasticity. If $x$ increases by 1%, $y$ increases by $\beta$%

-   Log-level regression: percentage change. If $x$ increases by 1, $y$ increases by $100\beta$

-   Level-log regression: level change. If $x$ increases by 1%, $y$ increases by $\beta/100$

## From the model to the data

### Solow model

-   Using Harrod-neutral technological progress, the GDP per capita equation is: $$\small
    \frac{Y(t)}{L(t)} = K(t)^\alpha (A(t) L(t))^{1-\alpha} \quad \alpha \in \ (0,1)
    $$

-   We assume an exogenous growth rate of $A$ and $L$ such that: $A(t) = A(0)e^{gt}$ and $L(t) = L(0) e^{nt}$.

-   At the steady-state level:

$$\small
\frac{Y}{L} = A(0)e^{gt}\left(\frac{s}{n + g + \delta}\right)^{\alpha/(1-\alpha)}
$$

-   Super convenient: we can measure everything
    -   $Y/L$, GDP divided by working age population
    -   $s$, saving measured as the real investment
    -   $n$, population growth rate
    -   $\delta$ and $g$, capital depreciation and growth rate of technology (assumed constant)

## From the model to the data

### Is this relationship linear?

**No**. Solution?

::: fragment
Log-linearize it (at time 0)

$$
\log (Y/L) = \log(A(0)) + \left(\frac{\alpha}{1-\alpha}\right)\log s - \left(\frac{\alpha}{1-\alpha}\right) \log(n + g + \delta)
$$
:::

## From the model to the data

### Empirical model

Because we do not observe all data, we can only *estimate* the parameters. Hence, the empirical model we estimate is:

$$
\log (Y_i/L_i) = \beta_0 + \beta_1 \log s_i + \beta_2 \log(n + g + \delta) + \epsilon_i
$$

where $\beta_0 + \epsilon_i = \log A(0)$, and $\epsilon_i$ is an error term capturing everything not captured in the model.

::: callout-note
We can, or not, assume that $\beta_1 = -\beta_2$. This is an empirical prediction we might want to test.
:::

## Exercise

### Data cleaning and descriptive statistics

1.  Load the data and the packages (`dplyr` and `ggplot2`)
2.  What are the three groups of countries in the data?
3.  Compute summary statistics based on country group
4.  Plot the growth rate of GDP per capita vs. the log of GDP per capita

## Exercise

### Baseline estimation

Assume $g+\delta=0.05$.

1.  Generate the variables you need in the regression
2.  Run the model on the full sample and on each sub-group. Store the results in appropriate objects.
    -   *\[Bonus\]* Use a loop to do it
3.  Interpret the results in the light of the Solow model predictions. What is the share of cross-country income per capita variation is explained by the model?
4.  *\[Bonus\]* We assumed that $\beta_1 \neq -\beta_2$. Using `linearHypothesis()` from the package `car`, you can set an hypothesis testing to check if the sum of the coefficient is equal to 0.
5.  In previous work, the share of capital in production was thought to be roughly 1/3, is this prediction supported by the data?

## Exercise

### Adding the human capital

1.  Does the previous model violate the exogeneity assumption? Why?
2.  A proposed solution is to add a new variable to capture the level of human capital: `school`.
3.  Run again the model on the full and sub- samples.
4.  Interpret.
5.  Export the tables and the graph in LaTeX

## Appendix {#appendix}

### Anscombe quartet

```{r}
g1 = ggplot(datasets::anscombe,aes(x=x1,y=y1)) +
  geom_point(color="black") +
  geom_smooth(method = "lm",se=F) +
  theme_bw() +
  xlab("X") + ylab("Y") + ggtitle("Q1")
g2 = ggplot(datasets::anscombe,aes(x=x2,y=y2)) +
  geom_point(color="black") +
  geom_smooth(method = "lm",se=F) +
  theme_bw() +
  xlab("X") + ylab("Y") + ggtitle("Q2")
g3 = ggplot(datasets::anscombe,aes(x=x3,y=y3)) +
  geom_point(color="black") +
  geom_smooth(method = "lm",se=F) +
  theme_bw() +
  xlab("X") + ylab("Y") + ggtitle("Q3")
g4 = ggplot(datasets::anscombe,aes(x=x4,y=y4)) +
  geom_point(color="black") +
  geom_smooth(method = "lm",se=F) +
  theme_bw() +
  xlab("X") + ylab("Y") + ggtitle("Q4")

(g1 | g2) /
(g3 | g4)
```